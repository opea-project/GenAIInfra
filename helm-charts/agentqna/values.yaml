# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

# Default values for agentqna.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext:
  readOnlyRootFilesystem: true
  allowPrivilegeEscalation: false
  runAsNonRoot: true
  runAsUser: 1000
  capabilities:
    drop:
    - ALL
  seccompProfile:
    type: RuntimeDefault

nodeSelector: {}

tolerations: []

affinity: {}

# This is just to avoid Helm errors when HPA is NOT used
# (use hpa-values.yaml files to actually enable HPA).
horizontalPodAutoscaler:
  enabled: false

docretriever:
  image:
    repository: opea/doc-index-retriever
    # Uncomment the following line to set desired image pull policy if needed, as one of Always, IfNotPresent, Never.
    # pullPolicy: ""
    # Overrides the image tag whose default is the chart appVersion.
    tag: "latest"

sqlagent:
  toolHostPath: "/mnt/tools"
  db_name: "Chinook"
  db_path: "sqlite:////home/user/tools/Chinook_Sqlite.sqlite"
  service:
    port: 9096
  strategy: sql_agent_llama
  with_memory: false
  use_hints: "false"
  recursion_limit: "6"
  llm_engine: vllm
  llm_endpoint_url: ""
  model: "meta-llama/Llama-3.3-70B-Instruct"
  temperature: "0"
  max_new_tokens: "4096"
  stream: "false"
  tools: ""
  require_human_feedback: "false"

ragagent:
  toolHostPath: "/mnt/tools"
  service:
    port: 9095
  strategy: rag_agent_llama
  with_memory: false
  recursion_limit: "6"
  llm_engine: vllm
  llm_endpoint_url: ""
  model: "meta-llama/Llama-3.3-70B-Instruct"
  temperature: "0"
  max_new_tokens: "4096"
  stream: "false"
  tools: "/home/user/tools/worker_agent_tools.yaml"
  require_human_feedback: "false"
  RETRIEVAL_TOOL_URL: ""

supervisor:
  toolHostPath: "/mnt/tools"
  service:
    port: 9090
  strategy: react_llama
  with_memory: true
  recursion_limit: 10
  llm_engine: vllm
  llm_endpoint_url: ""
  model: "meta-llama/Llama-3.3-70B-Instruct"
  temperature: "0"
  max_new_tokens: "4096"
  stream: "true"
  tools: /home/user/tools/supervisor_agent_tools.yaml
  require_human_feedback: false
  CRAG_SERVER: ""
  WORKER_AGENT_URL: ""
  SQL_AGENT_URL: ""

crag:
  image:
    repository: aicrowd/kdd-cup-24-crag-mock-api
    # Uncomment the following line to set desired image pull policy if needed, as one of Always, IfNotPresent, Never.
    # pullPolicy: ""
    # Overrides the image tag whose default is the chart appVersion.
    tag: "v0"

# Override values in specific subcharts
tgi:
  enabled: false

vllm:
  enabled: false

nginx:
  service:
    type: NodePort

agentqna-ui:
  image:
    repository: opea/agent-ui
    tag: "latest"
  BACKEND_SERVICE_ENDPOINT: "/v1/chat/completions"
  containerPort: 5173

global:
  http_proxy: ""
  https_proxy: ""
  no_proxy: ""
  HUGGINGFACEHUB_API_TOKEN: "insert-your-huggingface-token-here"
  # service account name to be shared with all parent/child charts.
  # If set, it will overwrite serviceAccount.name.
  # If set, and serviceAccount.create is false, it will assume this service account is already created by others.
  sharedSAName: "agentqna"
  # set modelUseHostPath or modelUsePVC to use model cache.
  modelUseHostPath: ""
  # modelUseHostPath: /mnt/opea-models
  # modelUsePVC: model-volume

  # Install Prometheus serviceMonitors for service components
  monitoring: false

  # Prometheus Helm install release name needed for serviceMonitors
  prometheusRelease: prometheus-stack
