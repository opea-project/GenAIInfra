# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

# Accelerate inferencing in heaviest components to improve performance
# by overriding their subchart values

tgi:
  enabled: false

vllm:
  enabled: true
  accelDevice: "gaudi"
  image:
    repository: opea/vllm-gaudi
    tag: "latest"
  resources:
    limits:
      habana.ai/gaudi: 1
  startupProbe:
    initialDelaySeconds: 5
    periodSeconds: 5
    timeoutSeconds: 1
    failureThreshold: 120
  readinessProbe:
    initialDelaySeconds: 5
    periodSeconds: 5
    timeoutSeconds: 1
  livenessProbe:
    initialDelaySeconds: 5
    periodSeconds: 5
    timeoutSeconds: 1

  # TODO: these are taken from GenAIExamples HPU manifest as-is
  # vLLM chart needs to adopt / apply relevant ones
  HABANA_LOGS: "/tmp/habana_logs"
  NUMBA_CACHE_DIR: "/tmp"
  PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
  OMPI_MCA_btl_vader_single_copy_mechanism: "none"
  HF_HOME: "/tmp/.cache/huggingface"
  GPU_MEMORY_UTILIZATION: "0.5"
  DTYPE: "auto"
  TENSOR_PARALLEL_SIZE: "1"
  BLOCK_SIZE: "128"
  MAX_NUM_SEQS: "256"
  MAX_SEQ_LEN_TO_CAPTURE: "2048"


# Reranking: second largest bottleneck when reranking is in use
# (i.e. query context docs have been uploaded with data-prep)
#
# TODO: could vLLM be used also for reranking / embedding?
teirerank:
  accelDevice: "gaudi"
  OMPI_MCA_btl_vader_single_copy_mechanism: "none"
  MAX_WARMUP_SEQUENCE_LENGTH: "512"
  image:
    repository: ghcr.io/huggingface/tei-gaudi
    tag: 1.5.0
  resources:
    limits:
      habana.ai/gaudi: 1
  securityContext:
    readOnlyRootFilesystem: false
  livenessProbe:
    timeoutSeconds: 1
  readinessProbe:
    timeoutSeconds: 1
