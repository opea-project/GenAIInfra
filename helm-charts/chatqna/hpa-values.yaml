# Copyright (C) 2024-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

# Enable HorizontalPodAutoscaler (HPA) for ChatQnA and its components
#
# Will create configMap with ChatQnA specific custom metric queries for embedding, reranking,
# and LLM inferencing services, which can be used to overwrite current PrometheusAdapter rules.
# This will then provide custom metrics used by HorizontalPodAutoscaler rules of each service.
#
# Default upstream adapter configMap is in:
#  - https://github.com/kubernetes-sigs/prometheus-adapter/blob/master/deploy/manifests/config-map.yaml

autoscaling:
  enabled: true
  minReplicas: 1
  maxReplicas: 4
  # ChatQnA becomes scaling bottleneck when gets close to 100% CPU usage
  targetCPUUtilizationPercentage: 80
resources:
  # CPU side HPA won't work without resource requests
  requests:
    # ChatQnA does not thread currently
    cpu: 1

global:
  # Both Grafana dashboards and k8s custom metrics need (Prometheus) metrics for services
  monitoring: true

# Override values in specific subcharts
#
# Note: enabling "autoscaling" for any of the subcharts requires enabling it also above!

dashboard:
  # add also scaling metrics dashboard to Grafana
  scaling: true

vllm:
  # vLLM startup takes too long for autoscaling, especially with Gaudi
  VLLM_SKIP_WARMUP: "true"
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 4
    activeRequestsTarget:
      accel: 120
      cpu: 10

tgi:
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 4
    queueSizeTarget:
      accel: 10
      cpu: 10

teirerank:
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 3
    queueSizeTarget:
      accel: 10
      cpu: 10

tei:
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 2
    queueSizeTarget:
      accel: 10
      cpu: 10
