# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

# Default values for chatqna.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: opea/chatqna
  # Uncomment the following line to set desired image pull policy if needed, as one of Always, IfNotPresent, Never.
  # pullPolicy: ""
  # Overrides the image tag whose default is the chart appVersion.
  tag: "latest"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext:
  readOnlyRootFilesystem: true
  allowPrivilegeEscalation: false
  runAsNonRoot: true
  runAsUser: 1000
  capabilities:
    drop:
    - ALL
  seccompProfile:
    type: RuntimeDefault

port: 8888
service:
  type: ClusterIP
  port: 8888

nodeSelector: {}

tolerations: []

affinity: {}

# This is just to avoid Helm errors when HPA is NOT used
# (use hpa-values.yaml files to actually enable HPA).
autoscaling:
  enabled: false

# Optional subcharts enablement and subcharts settings overwritten
# LLM choice, tgi by default.
tgi:
  enabled: false
  LLM_MODEL_ID: meta-llama/Meta-Llama-3-8B-Instruct
ollama:
  enabled: false
  LLM_MODEL_ID: llama3.2:1b
vllm:
  enabled: true
  LLM_MODEL_ID: meta-llama/Meta-Llama-3-8B-Instruct
  shmSize: 128Gi
  VLLM_TORCH_PROFILER_DIR: "/tmp/vllm_profile"
data-prep:
  # if set to false, need to set also nginx.enabled=false
  enabled: true
  # the following are for redis-vector-db
  DATAPREP_BACKEND: "REDIS"
  INDEX_NAME: "rag-redis"
  # the following are for qdrant db
  COLLECTION_NAME: "rag-qdrant"
  QDRANT_PORT: 6333
  # the following are for milvus db
  MILVUS_PORT: 19530
retriever-usvc:
  enabled: true
  # the following are for redis-vector-db
  RETRIEVER_BACKEND: "REDIS"
  INDEX_NAME: "rag-redis"
  # the following are for qdrant db
  QDRANT_INDEX_NAME: "rag-qdrant"
  QDRANT_PORT: 6333
  # the following are for milvus db
  MILVUS_PORT: 19530


# disable guardrails by default
# See guardrails-values.yaml for guardrail related options
guardrails-usvc:
  enabled: false
vllm-guardrails:
  enabled: false

# reranking
teirerank:
  enabled: true

# Text embedding inference service
tei:
  enabled: true

# vector db choice
# redis by default.
redis-vector-db:
  enabled: true
# qdrant, disabled by default
qdrant:
  enabled: false
  tag: "v1.13.1"
  config:
    cluster:
      enabled: false
milvus:
  enabled: false
  # Milvus config for standalone mode with no PVC which has minimum requirements for the K8s cluster.
  # Check https://github.com/zilliztech/milvus-helm/tree/milvus-4.2.12/charts/milvus for more production level configuration.
  cluster:
    enabled: false
  etcd:
    replicaCount: 1
    persistence:
      enabled: false
  pulsar:
    enabled: false
  minio:
    mode: standalone
    persistence:
      enabled: false
  standalone:
    persistence:
      enabled: false


# Microservice layer, disabled by default
llm-uservice:
  enabled: false
  TEXTGEN_BACKEND: "vLLM"
  LLM_MODEL_ID: meta-llama/Meta-Llama-3-8B-Instruct
embedding-usvc:
  enabled: false
  EMBEDDING_BACKEND: "TEI"
reranking-usvc:
  enabled: false
  RERANK_BACKEND: "TEI"

nginx:
  enabled: true
  service:
    type: NodePort

# UI configuration
chatqna-ui:
  # if set to false, need to set also nginx.enabled=false
  enabled: true
  image:
    repository: opea/chatqna-ui
    tag: "latest"
  containerPort: "5173"

dashboard:
  prefix: "OPEA ChatQnA"

# External inferencing configurations

externalAuthorization:
  needed: false
  # common authorization for all the external inferencing services
  OPENAI_API_KEY: "your-api-key"

externalEmbed:
  enabled: false
  EMBEDDING_SERVER_HOST: "http://your-embed-server"
  EMBEDDING_SERVER_PORT: "80"
  EMBEDDING_MODEL_ID: "your-embed-model"

externalRerank:
  enabled: false
  RERANK_SERVER_HOST: "http://your-rerank-server"
  RERANK_SERVER_PORT: "80"
  RERANK_MODEL_ID: "your-rerank-model"

externalGuardrails:
  enabled: false
  GUARDRAIL_SERVER_HOST: "http://your-guardrails-server"
  GUARDRAIL_SERVER_PORT: "80"
  GUARDRAIL_MODEL_ID: "your-guardrails-model"

externalLLM:
  enabled: false
  LLM_SERVER_HOST: "http://your-llm-server"
  LLM_SERVER_PORT: "80"
  LLM_MODEL: "your-model"
  OPENAI_API_KEY: "your-api-key"

global:
  http_proxy: ""
  https_proxy: ""
  no_proxy: ""
  HF_TOKEN: "insert-your-huggingface-token-here"
  # service account name to be shared with all parent/child charts.
  # If set, it will overwrite serviceAccount.name.
  # If set, and serviceAccount.create is false, it will assume this service account is already created by others.
  sharedSAName: "chatqna"
  # set modelUseHostPath or modelUsePVC to use model cache.
  modelUseHostPath: ""
  # modelUseHostPath: /mnt/opea-models
  # modelUsePVC: model-volume

  # Prometheus monitoring + Grafana dashboard(s) for service components?
  monitoring: false

  # Prometheus/Grafana namespace for Dashboard installation
  prometheusNamespace: monitoring

  # Prometheus Helm install release name needed for serviceMonitors
  prometheusRelease: prometheus-stack
