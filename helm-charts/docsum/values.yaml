# Copyright (C) 2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

# Default values for docsum.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: opea/docsum
  # Uncomment the following line to set desired image pull policy if needed, as one of Always, IfNotPresent, Never.
  # pullPolicy: ""
  # Overrides the image tag whose default is the chart appVersion.
  tag: "1.4"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext:
  readOnlyRootFilesystem: true
  allowPrivilegeEscalation: false
  runAsNonRoot: true
  runAsUser: 1000
  capabilities:
    drop:
    - ALL
  seccompProfile:
    type: RuntimeDefault

port: 8888
service:
  type: ClusterIP
  port: 8888

nodeSelector: {}

tolerations: []

affinity: {}

# To override values in subchart llm-uservice
# For external LLM configuration, you can override these values here or use variant_external-llm-values.yaml
# Example for external OpenAI-compatible endpoints:
#   DOCSUM_BACKEND: "vLLM"  # Use vLLM backend for OpenAI-compatible APIs
#   LLM_ENDPOINT: "https://api.openai.com"  # External endpoint (omit /v1 suffix)
#   LLM_MODEL_ID: "gpt-4-turbo"  # Model to use
#   OPENAI_API_KEY: "your-api-key"  # API key for authentication
llm-uservice:
  enabled: true
  image:
    repository: opea/llm-docsum
  DOCSUM_BACKEND: "vLLM"
  MAX_INPUT_TOKENS: "1024"
  MAX_TOTAL_TOKENS: "2048"
  LLM_MODEL_ID: meta-llama/Meta-Llama-3-8B-Instruct

# To override values in TGI/vLLM subcharts
tgi:
  enabled: false
  LLM_MODEL_ID: meta-llama/Meta-Llama-3-8B-Instruct
  MAX_INPUT_LENGTH: "1024"
  MAX_TOTAL_TOKENS: "2048"
vllm:
  enabled: true
  LLM_MODEL_ID: meta-llama/Meta-Llama-3-8B-Instruct

# Use docsum gradio UI
nginx:
  enabled: false
docsum-ui:
  # if false, set also nginx.enabled=false
  enabled: true
  image:
    repository: opea/docsum-gradio-ui
    tag: "1.4"
  containerPort: 5173
  service:
    type: NodePort

# NOTE: uncomment the following settings to use other docsum UI
# nginx:
#   enabled: true
#   service:
#     type: NodePort
# docsum-ui:
#   image:
#     repository: opea/docsum-ui
#     tag: "1.4"
#   BACKEND_SERVICE_ENDPOINT: "/v1/docsum"
#   containerPort: 5173
#   service:
#     type: ClusterIP

dashboard:
  enabled: true
  prefix: "OPEA DocSum"

whisper:
  enabled: true

global:
  http_proxy: ""
  https_proxy: ""
  no_proxy: ""
  HF_TOKEN: "insert-your-huggingface-token-here"
  # service account name to be shared with all parent/child charts.
  # If set, it will overwrite serviceAccount.name.
  # If set, and serviceAccount.create is false, it will assume this service account is already created by others.
  sharedSAName: "docsum"
  # set modelUseHostPath or modelUsePVC to use model cache.
  modelUseHostPath: ""
  # modelUseHostPath: /mnt/opea-models
  # modelUsePVC: model-volume

  # Prometheus monitoring + Grafana dashboard(s) for service components?
  monitoring: false

  # Prometheus/Grafana namespace for Dashboard installation
  prometheusNamespace: monitoring

  # Prometheus Helm install release name needed for serviceMonitors
  prometheusRelease: prometheus-stack
