# Copyright (C) 2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "ipexllm.fullname" . }}
  labels:
    {{- include "ipexllm.labels" . | nindent 4 }}
spec:
  {{- if ne (int .Values.replicaCount) 1 }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "ipexllm.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "ipexllm.labels" . | nindent 8 }}
        {{- with .Values.podLabels }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "ipexllm.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      {{- if not (hasPrefix "/data/" .Values.LLM_MODEL_ID) }}
      initContainers:
        - name: model-downloader
          envFrom:
            - configMapRef:
                name: {{ include "ipexllm.fullname" . }}-config
          securityContext:
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
              add:
              - DAC_OVERRIDE
              # To be able to make data model directory group writable for
              # previously downloaded model by old versions of helm chart
              - FOWNER
            seccompProfile:
              type: RuntimeDefault
          image: huggingface/downloader:0.17.3
          command: ['sh', '-ec']
          args:
            - |
              echo "Huggingface log in ...";
              huggingface-cli login --token $(HF_TOKEN);
              echo "Download model {{ .Values.LLM_MODEL_ID }} ... ";
              huggingface-cli download --cache-dir /data {{ .Values.LLM_MODEL_ID | quote }};
              echo "Change model files mode ...";
              chmod -R g+w /data/models--{{ replace "/" "--" .Values.LLM_MODEL_ID }}
              # NOTE: Buggy logout command;
              # huggingface-cli logout;
          volumeMounts:
            - mountPath: /data
              name: model-volume
            - mountPath: /tmp
              name: tmp
      {{- end }}
      containers:
        - name: {{ .Chart.Name }}
          envFrom:
            - configMapRef:
                name: {{ include "ipexllm.fullname" . }}-config
            {{- if .Values.global.extraEnvConfig }}
            - configMapRef:
                name: {{ .Values.global.extraEnvConfig }}
                optional: true
            {{- end }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          {{- if .Values.image.pullPolicy }}
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          {{- end }}
          command: ['bash', '-c']
          args:
            - |
              export CCL_WORKER_COUNT=2;
              export SYCL_CACHE_PERSISTENT=1;
              export FI_PROVIDER=shm;
              export CCL_ATL_TRANSPORT=ofi;
              export CCL_ZE_IPC_EXCHANGE=sockets;
              export CCL_ATL_SHM=1;
              export USE_XETLA=OFF;
              export SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=2;
              export TORCH_LLM_ALLREDUCE=0;
              export CCL_SAME_STREAM=1;
              export CCL_BLOCKING_WAIT=0;
              source /opt/intel/1ccl-wks/setvars.sh;
              exec python -m ipex_llm.vllm.xpu.entrypoints.openai.api_server \
                     --port 8000 \
                     --device xpu \
                     --enforce-eager \
                     --trust-remote-code \
                     --disable-async-output-proc \
                     --distributed-executor-backend ray \
                     --download-dir /data \
                     --served-model-name {{ trimPrefix "/data/" .Values.LLM_MODEL_ID }} \
                     --model {{ .Values.LLM_MODEL_ID }} \
                     --block-size {{ default 8 .Values.BLOCK_SIZE }} \
                     --gpu-memory-utilization {{ default 0.95 .Values.GPU_MEM_UTIL }} \
                     --dtype {{ default "float16" .Values.DTYPE }} \
                     --load-in-low-bit {{ default "fp8" .Values.QUANTIZATION }} \
                     --max-model-len {{ default 2000 .Values.MAX_MODEL_LEN }} \
                     --max-num-batched-tokens {{ default 3000 .Values.MAX_NUM_BATCHED_TOKENS }} \
                     --max-num-seqs {{ default 256 .Values.MAX_NUM_SEQS }} \
                     --tensor-parallel-size {{ default 1 .Values.TENSOR_PARALLEL_SIZE }}
                     --pipeline-parallel-size {{ default 1 .Values.PIPELINE_PARALLEL_SIZE }} \
                     {{- if .Values.extraCmdArgs }}
                     {{- join " " .Values.extraCmdArgs | nindent 21 }} \
                     {{- end }}
                     ;
          ports:
            - name: ipexllm
              containerPort: 8000
              protocol: TCP
          {{- if .Values.livenessProbe }}
          livenessProbe:
            {{- toYaml .Values.livenessProbe | nindent 12 }}
          {{- end }}
          {{- if .Values.readinessProbe }}
          readinessProbe:
            {{- toYaml .Values.readinessProbe | nindent 12 }}
          {{- end }}
          {{- if .Values.startupProbe }}
          startupProbe:
            {{- toYaml .Values.startupProbe | nindent 12 }}
          {{- end }}
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
          volumeMounts:
            - mountPath: /data
              name: model-volume
            {{- if .Values.shmSize }}
            - mountPath: /dev/shm
              name: shm
            {{- end }}
            - mountPath: /tmp
              name: tmp
      volumes:
        - name: model-volume
          {{- if .Values.global.modelUsePVC }}
          persistentVolumeClaim:
            claimName: {{ .Values.global.modelUsePVC }}
          {{- else if .Values.global.modelUseHostPath }}
          hostPath:
            path: {{ .Values.global.modelUseHostPath }}
            type: Directory
          {{- else }}
          emptyDir: {}
          {{- end }}
        {{- if .Values.shmSize }}
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: {{ .Values.shmSize }}
        {{- end }}
        - name: tmp
          emptyDir: {}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- if .Values.evenly_distributed }}
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              {{- include "ipexllm.selectorLabels" . | nindent 14 }}
      {{- end }}
      {{- if not .Values.accelDevice }}
      # extra time to finish processing buffered requests on CPU before pod is forcibly terminated
      terminationGracePeriodSeconds: 120
      {{- end }}
