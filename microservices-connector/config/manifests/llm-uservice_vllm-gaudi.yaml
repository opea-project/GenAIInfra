---
# Source: llm-uservice/charts/vllm/templates/configmap.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-uservice-vllm-config
  labels:
    helm.sh/chart: vllm-1.3.0
    app.kubernetes.io/name: vllm
    app.kubernetes.io/instance: llm-uservice
    app.kubernetes.io/version: "0.5"
    app.kubernetes.io/managed-by: Helm
data:
  HF_TOKEN: "insert-your-huggingface-token-here"
  http_proxy: ""
  https_proxy: ""
  no_proxy: ""
  HABANA_LOGS: "/tmp/habana_logs"
  NUMBA_CACHE_DIR: "/tmp"
  HF_HOME: "/tmp/.cache/huggingface"
  XDG_CONFIG_HOME: "/tmp"
  TORCHINDUCTOR_CACHE_DIR: "/tmp/pytorchinductor_cache"
  # https://github.com/outlines-dev/outlines/blob/main/outlines/caching.py#L14-L29
  OUTLINES_CACHE_DIR: "/tmp/.cache/outlines"
  OMPI_MCA_btl_vader_single_copy_mechanism: "none"
---
# Source: llm-uservice/templates/configmap.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-uservice-config
  labels:
    helm.sh/chart: llm-uservice-1.3.0
    app.kubernetes.io/name: llm-uservice
    app.kubernetes.io/instance: llm-uservice
    app.kubernetes.io/version: "v1.0"
    app.kubernetes.io/managed-by: Helm
data:
  LLM_COMPONENT_NAME: "OpeaTextGenService"
  LLM_ENDPOINT: "http://llm-uservice-vllm"
  LLM_MODEL_ID: "Intel/neural-chat-7b-v3-3"
  HF_HOME: "/tmp/.cache/huggingface"
  HF_TOKEN: "insert-your-huggingface-token-here"
  http_proxy: ""
  https_proxy: ""
  no_proxy: ""
  LOGFLAG: ""
---
# Source: llm-uservice/charts/vllm/templates/service.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: v1
kind: Service
metadata:
  name: llm-uservice-vllm
  labels:
    helm.sh/chart: vllm-1.3.0
    app.kubernetes.io/name: vllm
    app.kubernetes.io/instance: llm-uservice
    app.kubernetes.io/version: "0.5"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 2080
      protocol: TCP
      name: vllm
  selector:
    app.kubernetes.io/name: vllm
    app.kubernetes.io/instance: llm-uservice
---
# Source: llm-uservice/templates/service.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: v1
kind: Service
metadata:
  name: llm-uservice
  labels:
    helm.sh/chart: llm-uservice-1.3.0
    app.kubernetes.io/name: llm-uservice
    app.kubernetes.io/instance: llm-uservice
    app.kubernetes.io/version: "v1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 9000
      targetPort: 9000
      protocol: TCP
      name: llm-uservice
  selector:
    app.kubernetes.io/name: llm-uservice
    app.kubernetes.io/instance: llm-uservice
---
# Source: llm-uservice/charts/vllm/templates/deployment.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-uservice-vllm
  labels:
    helm.sh/chart: vllm-1.3.0
    app.kubernetes.io/name: vllm
    app.kubernetes.io/instance: llm-uservice
    app.kubernetes.io/version: "0.5"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: vllm
      app.kubernetes.io/instance: llm-uservice
  template:
    metadata:
      
      labels:
        app.kubernetes.io/name: vllm
        app.kubernetes.io/instance: llm-uservice
    spec:      
      serviceAccountName: default
      securityContext:
        {}
      initContainers:
        - name: model-downloader
          envFrom:
            - configMapRef:
                name: llm-uservice-vllm-config
          securityContext:
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
              add:
              - DAC_OVERRIDE
              # To be able to make data model directory group writable for
              # previously downloaded model by old versions of helm chart
              - FOWNER
            seccompProfile:
              type: RuntimeDefault
          image: huggingface/downloader:0.17.3
          command: ['sh', '-ec']
          args:
            - |
              echo "Huggingface log in ...";
              huggingface-cli login --token $(HF_TOKEN);
              echo "Download model Intel/neural-chat-7b-v3-3 ... ";
              huggingface-cli download --cache-dir /data "Intel/neural-chat-7b-v3-3";
              echo "Change model files mode ...";
              chmod -R g+w /data/models--Intel--neural-chat-7b-v3-3;
              # Some models(e.g. mistralai/Mistral-7B-Instruct-v0.3, etc) need to write additional files to /data root directory
              chmod -R g+w /data;
              chmod -R g+w $(HF_HOME);
              # NOTE: Buggy logout command;
              # huggingface-cli logout;
          volumeMounts:
            - mountPath: /data
              name: model-volume
            - mountPath: /tmp
              name: tmp
      containers:
        - name: vllm
          envFrom:
            - configMapRef:
                name: llm-uservice-vllm-config
            - configMapRef:
                name: extra-env-config
                optional: true
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: false
            runAsUser: 0
            seccompProfile:
              type: RuntimeDefault
          image: "opea/vllm-gaudi:1.3"
          args:
            - "--tensor-parallel-size"
            - "1"
            - "--block-size"
            - "128"
            - "--max-num-seqs"
            - "256"
            - "--max-seq-len-to-capture"
            - "2048"
            - "--model"
            - "Intel/neural-chat-7b-v3-3"
            - "--host"
            - "0.0.0.0"
            - "--port"
            - "2080"
            - "--download-dir"
            - "/data"
          volumeMounts:
            - mountPath: /data
              name: model-volume
            - mountPath: /dev/shm
              name: shm
            - mountPath: /tmp
              name: tmp
          ports:
            - name: http
              containerPort: 2080
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 5
            periodSeconds: 5
          startupProbe:
            failureThreshold: 360
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 5
            periodSeconds: 5
          resources:
            limits:
              habana.ai/gaudi: 1
            requests:
              cpu: 100m
              memory: 128Mi
      volumes:
        - name: model-volume
          hostPath:
            path: /mnt/opea-models
            type: Directory
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 1Gi
        - name: tmp
          emptyDir: {}
      # extra time to finish processing buffered requests on CPU before pod is forcibly terminated
      terminationGracePeriodSeconds: 120
---
# Source: llm-uservice/templates/deployment.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-uservice
  labels:
    helm.sh/chart: llm-uservice-1.3.0
    app.kubernetes.io/name: llm-uservice
    app.kubernetes.io/instance: llm-uservice
    app.kubernetes.io/version: "v1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: llm-uservice
      app.kubernetes.io/instance: llm-uservice
  template:
    metadata:
      
      labels:
        app.kubernetes.io/name: llm-uservice
        app.kubernetes.io/instance: llm-uservice
    spec:      
      serviceAccountName: default
      securityContext:
        {}
      initContainers:
        - name: wait-for-llm
          envFrom:
            - configMapRef:
                name: llm-uservice-config
            - configMapRef:
                name: extra-env-config
                optional: true
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
            seccompProfile:
              type: RuntimeDefault
          image: busybox:1.36
          command: ["sh", "-c"]
          args:
            - |
              proto=$(echo ${LLM_ENDPOINT} | sed -n 's/.*\(http[s]\?\):\/\/\([^ :]\+\):\?\([0-9]*\).*/\1/p');
              host=$(echo ${LLM_ENDPOINT} | sed -n 's/.*\(http[s]\?\):\/\/\([^ :]\+\):\?\([0-9]*\).*/\2/p');
              port=$(echo ${LLM_ENDPOINT} | sed -n 's/.*\(http[s]\?\):\/\/\([^ :]\+\):\?\([0-9]*\).*/\3/p');
              if [ -z "$port" ]; then
                  port=80;
                  [[ "$proto" = "https" ]] && port=443;
              fi;
              wait_timeout=1;
              total_timeout=720;
              j=0;
              while ! nc -w ${wait_timeout} -z ${host} ${port}; do
                j=$((j+wait_timeout));
                [[ $j -ge ${total_timeout} ]] && echo "ERROR: ${host}:${port} is NOT reachable in $j seconds!" && exit 1;
                j=$((j+2)); sleep 2;
              done;
              echo "${host}:${port} is reachable within $j seconds.";
      containers:
        - name: llm-uservice
          envFrom:
            - configMapRef:
                name: llm-uservice-config
            - configMapRef:
                name: extra-env-config
                optional: true
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1000
            seccompProfile:
              type: RuntimeDefault
          image: "opea/llm-textgen:1.3"
          ports:
            - name: llm-uservice
              containerPort: 9000
              protocol: TCP
          volumeMounts:
            - mountPath: /tmp
              name: tmp
          readinessProbe:
            httpGet:
              path: v1/health_check
              port: llm-uservice
            initialDelaySeconds: 5
            periodSeconds: 5
          startupProbe:
            failureThreshold: 120
            httpGet:
              path: v1/health_check
              port: llm-uservice
            initialDelaySeconds: 5
            periodSeconds: 5
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
      volumes:
        - name: tmp
          emptyDir: {}
---
# Source: llm-uservice/charts/vllm/templates/horizontal-pod-autoscaler.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0
---
# Source: llm-uservice/charts/vllm/templates/serviceaccount.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0
---
# Source: llm-uservice/charts/vllm/templates/servicemonitor.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0
#
# Dashboard for the exposed vLLM metrics:
# - https://github.com/vllm-project/vllm/tree/main/examples/production_monitoring/
# Metric descriptions:
# - https://docs.vllm.ai/en/stable/serving/metrics.html
---
# Source: llm-uservice/templates/serviceaccount.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0
---
# Source: llm-uservice/templates/servicemonitor.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0
