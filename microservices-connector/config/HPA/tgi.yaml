---
# Source: tgi/templates/configmap.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: v1
kind: ConfigMap
metadata:
  name: tgi-config
  labels:
    helm.sh/chart: tgi-0.8.0
    app.kubernetes.io/name: tgi
    app.kubernetes.io/instance: tgi
    app.kubernetes.io/version: "2.1.0"
    app.kubernetes.io/managed-by: Helm
data:
  MODEL_ID: "Intel/neural-chat-7b-v3-3"
  PORT: "2080"
  HF_TOKEN: "insert-your-huggingface-token-here"
  http_proxy: ""
  https_proxy: ""
  no_proxy: ""
  HABANA_LOGS: "/tmp/habana_logs"
  NUMBA_CACHE_DIR: "/tmp"
  TRANSFORMERS_CACHE: "/tmp/transformers_cache"
  HF_HOME: "/tmp/.cache/huggingface"
  CUDA_GRAPHS: "0"
---
# Source: tgi/templates/service.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: v1
kind: Service
metadata:
  name: tgi
  labels:
    helm.sh/chart: tgi-0.8.0
    app.kubernetes.io/name: tgi
    app.kubernetes.io/instance: tgi
    app.kubernetes.io/version: "2.1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 2080
      protocol: TCP
      name: tgi
  selector:
    app.kubernetes.io/name: tgi
    app.kubernetes.io/instance: tgi
---
# Source: tgi/templates/deployment.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: apps/v1
kind: Deployment
metadata:
  name: tgi
  labels:
    helm.sh/chart: tgi-0.8.0
    app.kubernetes.io/name: tgi
    app.kubernetes.io/instance: tgi
    app.kubernetes.io/version: "2.1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  # use explicit replica counts only of HorizontalPodAutoscaler is disabled
  selector:
    matchLabels:
      app.kubernetes.io/name: tgi
      app.kubernetes.io/instance: tgi
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tgi
        app.kubernetes.io/instance: tgi
    spec:
      securityContext:
        {}
      containers:
        - name: tgi
          envFrom:
            - configMapRef:
                name: tgi-config
            - configMapRef:
                name: extra-env-config
                optional: true
          securityContext:
            {}
          image: "ghcr.io/huggingface/text-generation-inference:2.2.0"
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - mountPath: /data
              name: model-volume
            - mountPath: /tmp
              name: tmp
          ports:
            - name: http
              containerPort: 2080
              protocol: TCP
          livenessProbe:
            failureThreshold: 24
            initialDelaySeconds: 5
            periodSeconds: 5
            tcpSocket:
              port: http
          readinessProbe:
            initialDelaySeconds: 5
            periodSeconds: 5
            tcpSocket:
              port: http
          startupProbe:
            failureThreshold: 120
            initialDelaySeconds: 5
            periodSeconds: 5
            tcpSocket:
              port: http
          resources:
            {}
      volumes:
        - name: model-volume
          hostPath:
            path: /mnt/opea-models
            type: Directory
        - name: tmp
          emptyDir: {}
      # extra time to finish processing buffered requests before HPA forcibly terminates pod
      terminationGracePeriodSeconds: 120
---
# Source: tgi/templates/horizontalPorAutoscaler.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: tgi
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: tgi
  minReplicas: 1
  maxReplicas: 6
  metrics:
  - type: Object
    object:
      metric:
        # TGI time metrics are in seconds
        name: tgi_request_latency
      describedObject:
        apiVersion: v1
        # get metric for named object of given type (in same namespace)
        kind: Service
        name: tgi
      target:
        # tgi_request_latency is average for all the TGI pods. To avoid replica fluctuations when
        # TGI startup + request processing takes longer than HPA evaluation period, this uses
        # "Value" (replicas = metric.value / target.value), instead of "averageValue" type:
        #  https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#algorithm-details
        type: Value
        value: 4
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 180
      policies:
      - type: Percent
        value: 25
        periodSeconds: 15
    scaleUp:
      selectPolicy: Max
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 50
        periodSeconds: 15
      - type: Pods
        value: 2
        periodSeconds: 15
---
# Source: tgi/templates/servicemonitor.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0
#
# Dashboard for the exposed TGI metrics:
# - https://grafana.com/grafana/dashboards/19831-text-generation-inference-dashboard/
# Metric descriptions:
# - https://github.com/huggingface/text-generation-inference/discussions/1127#discussioncomment-7240527
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: tgi
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: tgi
      app.kubernetes.io/instance: tgi
  endpoints:
  - interval: 4s
    port: tgi
    scheme: http
