---
# Source: llm-uservice/charts/tgi/templates/service.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: v1
kind: Service
metadata:
  name: llm-tgi
  labels:
    helm.sh/chart: tgi-0.1.0
    app.kubernetes.io/name: tgi
    app.kubernetes.io/instance: llm
    app.kubernetes.io/version: "1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: tgi
  selector:
    app.kubernetes.io/name: tgi
    app.kubernetes.io/instance: llm
---
apiVersion: v1
kind: Service
metadata:
  name: llm-llm-uservice
  labels:
    helm.sh/chart: llm-uservice-0.1.0
    app.kubernetes.io/name: llm-uservice
    app.kubernetes.io/instance: llm
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 9000
      targetPort: 9000
      protocol: TCP
      name: llm-uservice
  selector:
    app.kubernetes.io/name: llm-uservice
    app.kubernetes.io/instance: llm
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-tgi
  labels:
    helm.sh/chart: tgi-0.1.0
    app.kubernetes.io/name: tgi
    app.kubernetes.io/instance: llm
    app.kubernetes.io/version: "1.4"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: tgi
      app.kubernetes.io/instance: llm
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tgi
        app.kubernetes.io/instance: llm
    spec:
      securityContext:
        {}
      containers:
        - name: tgi
          env:
            - name: MODEL_ID
              value: HuggingFaceH4/mistral-7b-grok
            - name: PORT
              value: "80"
            - name: http_proxy
              value:
            - name: https_proxy
              value:
            - name: no_proxy
              value:
          securityContext:
            {}
          image: "ghcr.io/huggingface/text-generation-inference:1.4"
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - mountPath: /data
              name: model-volume
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          resources:
            {}
      volumes:
        - name: model-volume
          hostPath:
            path: /mnt
            type: Directory
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-llm-uservice
  labels:
    helm.sh/chart: llm-uservice-0.1.0
    app.kubernetes.io/name: llm-uservice
    app.kubernetes.io/instance: llm
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: llm-uservice
      app.kubernetes.io/instance: llm
  template:
    metadata:
      labels:
        app.kubernetes.io/name: llm-uservice
        app.kubernetes.io/instance: llm
    spec:
      securityContext:
        {}
      containers:
        - name: llm
          env:
            - name: TGI_LLM_ENDPOINT
              value: "http://llm-tgi:80"
            - name: HUGGINGFACEHUB_API_TOKEN
              value: "insert-your-huggingface-token-here"
            - name: http_proxy
              value:
            - name: https_proxy
              value:
            - name: no_proxy
              value:

          securityContext:
            {}
          image: "opea/llm-tgi:latest"
          imagePullPolicy: IfNotPresent
          ports:
            - name: llm-uservice
              containerPort: 9000
              protocol: TCP
          startupProbe:
            exec:
              command:
              - curl
              - http://llm-tgi:80
            initialDelaySeconds: 5
            periodSeconds: 5
            failureThreshold: 120
          resources:
            {}
